{% extends "base.html" %}

{% block head %}
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="{{ url_for('static', filename='css/index.css') }}">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

{% endblock %}

{% block content %}
<main role="main">

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <section class="jumbotron text-center">
      <div class="container">
        <h1 class="display-3">Seal Scripts</h1>
      </div>
    </section>

    <div class="py-5">
        <div class="container">
            <h1>Text-to-Speech Synthesis</h1>
            <p class="lead">Text-to-Speech synthesis was a completely new area for us! We decided to tackle this challenge in hopes of producing synthesized speech from our generated transcripts in the style of our targeted speakers. So, we researched and tested several different text-to-speech models.</p>
            <p class="lead">Overall, we learned that text-to-speech synthesis generally follows a three part model. First, the text needs to be embedded using an encoder. Then, these embeddings are passed to a synthesizer that infers mel spectrograms from the word embeddings. Since we didn’t have much experience with sound, <a href="https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53" target="_blank">wrapping our heads around the idea of a mel spectrogram was pretty difficult</a>, but in essence, a mel spectrogram encodes signal frequencies from audio over time using the mel scale. Finally, the mel spectrograms are passed through a vocoder, which generates waveforms from the spectrograms, producing audio!</p>
            <p class="lead">Below are the results of our journey!</p>
            
            <br>
            <h2>Experiment 1: <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" target="_blank">Real Time Voice Cloning</a></h2>
            <p class="lead">We started by working with CorentinJ’s very popular Real-Time-Voice-Cloning repo. This repository was an implementation of transfer learning for a text-to-speech model. We iterated on the Jupyter notebook in this repository and tried training on clips of our speakers for voice cloning.</p>
            <p class="lead">Initially, we hypothesized that attempting to voice clone using more data would lead to better results. So, we scraped audio from Stuart Reges’s CSE 143 lectures in Winter 2020. Not only was working with these massive .wav files exceptionally slow and memory-consuming (we had 30+ hours of data), but we discovered that this model (and most other models) were optimized to train on short ~10 second clips rather than long chunks of voice.</p>
            <p class="lead">After downsampling a 10 second clip of Stuart’s voice to 48 kHz (to match the pretrained model), we attempted to generate his iconic lecture intro. While the generated voice sounded vaguely like Stuart, the output often had odd silences and white noise interruptions that were distracting. This was a great start, but the speech didn’t sound quite realistic for our liking. Since we had so much more data, we decided to look for ways that we could train a model using more than a 10 second clip to see if we could get improved results.</p>
        </div>
    </div>

</main>

<script>
    window.addEventListener('scroll',function() {
        //When scroll change, you save it on localStorage.
        localStorage.setItem('scrollPosition',window.scrollY);
    },false);

    window.addEventListener('load',function() {
        if(localStorage.getItem('scrollPosition') !== null)
           window.scrollTo(0, localStorage.getItem('scrollPosition'));
    },false);
</script>

<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>
{% endblock %}
