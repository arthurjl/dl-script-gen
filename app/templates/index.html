{% extends "base.html" %}

{% block head %}
    <!-- Required meta tags -->
    
    <link rel="stylesheet" href="{{ url_for('static', filename='css/index.css') }}">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

{% endblock %}

{% block content %}
<main role="main">

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <section class="jumbotron text-center">
      <div class="container">
        <h1 class="display-3">Speech-to-Speech:</h1>
        <h3 class="display-4">Realistic Audio Reading of <br>Generated Transcripts</h2>
        <br>
        <p>Arthur Liu, Taylor Ka, Daniel Zhu</p>
      </div>
    </section>

    <div class="container">
        <div class="row">
            <div class="col-3">
                <div class="nav flex-column nav-pills" id="v-pills-tab" role="tablist" aria-orientation="vertical">
                    <a class="nav-link active" id="v-pills-demo-tab" data-toggle="pill" href="#v-pills-demo" role="tab" aria-controls="v-pills-demo" aria-selected="true">1 Demo</a>
                    <a class="nav-link" id="v-pills-overview-tab" data-toggle="pill" href="#v-pills-overview" role="tab" aria-controls="v-pills-overview" aria-selected="false">2 Overview/Video</a>
                    <a class="nav-link" id="v-pills-dataset-tab" data-toggle="pill" href="#v-pills-dataset" role="tab" aria-controls="v-pills-dataset" aria-selected="false">3 Dataset Creation</a>
                    <a class="nav-link" id="v-pills-language-tab" data-toggle="pill" href="#v-pills-language" role="tab" aria-controls="v-pills-language" aria-selected="false">4 Language Models</a>
                    <a class="nav-link" id="v-pills-speech-tab" data-toggle="pill" href="#v-pills-speech" role="tab" aria-controls="v-pills-speech" aria-selected="false">5 Speech Synthesis </a>
                </div>
            </div>
    
            <div class="col-9">
                <div class="tab-content" id="v-pills-tabContent">
                    <div class="tab-pane fade show active" id="v-pills-demo" role="tabpanel" aria-labelledby="v-pills-demo-tab">
                        <div>
                            <p class="lead">
                                Now that the quarter is almost over... aren't you going to miss listening to all your professors lecture? No? Maybe you've just watched all the Youtube videos
                                from your favorite vlogger? Are you sitting there pulling down to refresh waiting for new content? <b>Well regardless... you don't have to miss a beat!</b>  
                            </p>
                            <p class="lead">
                                Speech-to-Speech generates new video scripts and visualizes a little of what the model is seeing.
                            </p>
                            <h3>Try it yourself!</h3>
                            <br>
                            <form>
                                <div class="form-group lead">
                                    <label for="seedWords">Seed Words</label>
                                    <input type="text" class="form-control" name="seedWords" id="seedWords" placeholder="Enter seed words">
                                </div>
                                {% if error_message %}
                                <p style="color: red"><i>{{ error_message }}</i></p>
                                {% endif %}
                                <div class="form-row">
                                    <div class="col">
                                        <p class="lead">Text Generator</p>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="markRober" value="mark">
                                            <label class="form-check-label" for="markRober">
                                              Mark Rober
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="caseyNeistat" value="casey">
                                            <label class="form-check-label" for="caseyNeistat">
                                              Casey Neistat Vlogs
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="office" value="office" checked>
                                            <label class="form-check-label" for="office">
                                              The Office
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="documentary" value="documentary">
                                            <label class="form-check-label" for="documentary">
                                              PBS Documentaries
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="nature" value="nature">
                                            <label class="form-check-label" for="nature">
                                              Nature Documentary
                                            </label>
                                        </div>
                                    </div>
                                    <!-- Removed since audio is not hooked up -->
                                    <!-- <div class="col">
                                        <p class="lead">Audio Reader</p>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="audioReader" id="stuartRegesVoice" value="stuartRegesVoice">
                                            <label class="form-check-label" for="stuartRegesVoice">
                                              Stuart Reges
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="audioReader" id="caseyNeistatVoice" value="caseyNeistatVoice">
                                            <label class="form-check-label" for="caseyNeistatVoice">
                                              Casey Neistat
                                            </label>
                                        </div>
                                    </div> -->
                                </div>
                                <br>
                                <button type="submit" class="btn btn-primary">Generate</button>
                            </form>
            
                            <br>
                            {% if text %}
                                <h3>Generated Text</h3>
                                {% for p in text %}
                                    <p class="lead">{{ p }}</p>
                                {% endfor %}
                            {% endif %}
<!--                     
                            {% if audio %}
                                <h3>Audio Reading</h3>
                                <p class="lead">
                                    <audio controls>
                                        <source src="{{ audio }}" type="audio/wav">
                                    </audio>
                                </p>
                            {% endif %} -->
                    
                            {% if visualizations %}
                                <h3>
                                    Visualizatizing the Neurons
                                </h3>
                                <p class="lead">
                                    How individual cells contribute to the predictions - Red = Higher Activity, Blue = Mostly Inactive
                                </p>
                                {% for visualization in visualizations %}
                                <p class="lead">
                                    Cell #{{ visualization[0] }}
                                </p>
                                <p class="lead">
                                    {{ visualization[1]|safe}}
                                </p>
                                {% endfor %}
                            {% endif %}
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-overview" role="tabpanel" aria-labelledby="v-pills-overview-tab">
                        <div>
                            <h1>Hi!</h1>
                            <p class="lead">Welcome to seal-scripts, our AU 2020 Deep Learning project!</p>
                            <p class="lead">Well, don't take it from us, take it from Stuart Reges:</p>
                            <audio controls>
                                <source src="static/audio-examples/seal-scripts-intro.wav" type="audio/wav">
                            </audio>
                            <p class="lead">
                                Or maybe do take it from us :))
                                <div class="embed-responsive embed-responsive-16by9">
                                    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/n-aoyOrLqfE" allowfullscreen></iframe>
                                </div>
                            </p>
                            <br>
                            <br>
                            <p class="lead">The goal of our project was to survey two interesting categories of DL. The first is text data generation, and the second is audio data generation. We intended to combine both of these together with a silly idea of generating random fake audio clips from these generated texts.</p>
                            <p class="lead">We had to curate data and then preprocess and train on these pieces. Overall, here's what that looks like!</p>
                            <!-- Created and copied from draw io -->
                            <div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile modified=\&quot;2020-12-13T13:19:38.832Z\&quot; host=\&quot;app.diagrams.net\&quot; agent=\&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\&quot; etag=\&quot;YQCihewNGoS7LejKZ34G\&quot; version=\&quot;14.0.1\&quot; type=\&quot;google\&quot;&gt;&lt;diagram id=\&quot;Ht1M8jgEwFfnCIfOTk4-\&quot; name=\&quot;Page-1\&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
                            <p class="lead"> In the following sections, we'll go into more detail about each of the 3 sections: dataset creation, text generation, and speech synthesis. Finally, we will discuss our results
                                and compare the different models we experimented with.
                            </p>
                            <br>
                            <p class="lead">If you're wondering why we're called seal-scripts, for the models that we trained, it's probably a little dubious that a human generated the text transcripts... but it's plausible a smart seal could have written the text instead. But anyway, seals <i>are</i> pretty cute. ⭐</p>
                            <br>
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-dataset" role="tabpanel" aria-labelledby="v-pills-dataset-tab">
                        <div>
                            <h1>Dataset Creation</h1>
                            <p class="lead">The fun of this project comes from being able to style both the text generation and speech synthesis to people that we were familiar with. So, we collected this data online
                                from documentaries, Youtube, and online courseware. Sometimes, it was difficult to ensure that the transcripts of the videos we found were high quality, this was especially true
                                when we used auto-generated English captions from Youtube videos. All in all, we created a few datasets listed below:
                            <div class="col-8 mx-auto">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th scope="col">Name</th>
                                            <th scope="col">Source</th>
                                            <th scope="col">Size</th>
                                        </tr>
                                        <tbody>
                                            <tr>
                                                <td>Casey Neistat Transcripts</td>
                                                <td>YouTube</td>
                                                <td>1 mb</td>
                                            </tr>
                                            <tr>
                                                <td>Mark Rober Transcripts</td>
                                                <td>YouTube</td>
                                                <td>.75 mb</td>
                                            </tr>
                                            <tr>
                                                <td>PBS Nova Documentaries</td>
                                                <td><a href="https://www.pbs.org/wgbh/nova/transcripts" target="_blank">PBS site</a></td>
                                                <td>9 mb</td>
                                            </tr>
                                            <tr>
                                                <td>The Office Transcript</td>
                                                <td><a href="https://github.com/brianbuie/the-office" target="_blank">GitHub</a></td>
                                                <td>4 mb</td>
                                            </tr>
                                            <tr>
                                                <td>Stuart Reges Audio</td>
                                                <td>Panopto Lectures</td>
                                                <td>>30 hrs</td>
                                            </tr>
                                            <tr>
                                                <td>Kurzgesagt</td>
                                                <td>YouTube</td>
                                                <td>3 hrs</td>
                                            </tr>
                                        </tbody>
                                    </thead>
                                </table>
                            </div>
                            <h2>Text Datasets</h2>
                            <h3>YouTube</h3>
                            <p class="lead">
                                We initially wanted to make parodies of popular YouTube vloggers, so we needed to get transcripts from their videos. Using an existing python library, we could download all the subs of all the videos in a playlist, and we could simply use the “uploads” playlist to collect all of it. We also wrote <a href="https://drive.google.com/file/d/1EUgNoaEzcn-aGrVyOqM85s5Npi3dR35g/view?usp=sharing" target="_blank">a script</a> to clean up these .vtt files into a .txt file for our models to utilize.
                            </p>
                            <p class="lead">
                                Unfortunately, we noticed that they only had auto-generated captions, and so this significantly reduced the quality of our dataset. Additionally, the quantity of these captions was not very impressive (<< 10 mb), so we looked into other sources of data.
                            </p>
                            <h3>Other Datasets</h3>
                            <p class="lead">
                                Since the YouTube datasets were not sufficient enough, we also scraped data from other sources. We were able to utilize <a href="https://github.com/brianbuie/the-office" target="_blank">The Office json clips github here</a>, and then we <a href="https://drive.google.com/file/d/1PB06JIlFz5z380JYDRIfbnbA-WD5KWCI/view?usp=sharing" target="_blank">wrote a script</a> to convert the json files into a singular text file as well.
                            </p>
                            <p class="lead">
                                Another source of data was the <a href="https://www.pbs.org/wgbh/nova/transcripts" target="_blank">PBS NOVA transcripts here</a>. We wrote a <a href="https://drive.google.com/file/d/1nbzp8Z-5UMqoP7YGUgE8_4sej_MpOP5f/view?usp=sharing" target="_blank">web crawler</a> to scrape and clean these and also created individual files for specific domains of documentaries (ex: nature)
                            </p>
                            <br>
                            <h2>Audio Datasets</h2>
                            <h3>Stuart Reges</h3>
                            <p class="lead">
                                We thought it would be fun to have Stuart Reges’s voice read our transcripts since he’s such an iconic voice to many CS students at UW!
                            </p>
                            <p class="lead">
                                We <a href="https://colab.research.google.com/drive/1Rm7aSwkkpSLNcKL2M5kZsP74OLX_WEAU?usp=sharing" target="_blank">scraped all the audio</a> from the CSE 143 lectures in Winter 2020 from Panopto. This ended up being over 30 hours of data! Unfortunately, we weren’t able to train extensively on this full dataset since we didn’t have text transcriptions, and only ended up using short clips for voice cloning. We also hypothesize that the quality of the data might have been degraded by pauses in the lecture when students were asking questions, etc. 
                            </p>

                            <h3>Kurzgesagt</h3>
                            <p class="lead">
                                We picked Kurzgesagt because we love their videos, and they have human generated captions! These were crucial in order to have good transcriptions for each clip to train on. 
                            </p>
                            <p class="lead">
                                We <a href="https://colab.research.google.com/drive/1I-wbH6GeaSooRrgOWzupAg-6zD0nK807?usp=sharing" target="_blank">scraped a small subset of their videos</a> (about 3 hours and 17 minutes total), parsed the subtitle files, and used ffmpeg to create clips cut at the caption timestamps.
                            </p>
                            <p class="lead">
                                We also attempted to scrape all their videos (about 16 hours, 36 minutes total). Our first mistake was putting all our clips in one folder, which led to Google Drive timeout issues. After splitting up the clips into multiple folders, we solved the data loader issues, but encountered runtime errors while training likely due to malformed data. Unfortunately we were never quite able to debug these, so we stuck to our original sample size. 
                            </p>
                            <br>
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-language" role="tabpanel" aria-labelledby="v-pills-language-tab">
                        <div>
                            <h1>Text Generation</h1>
                            <p class="lead">We were largely inspired by Kaparthy’s article here: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">"The Unreasonable Effectiveness of Recurrent Neural Networks"</a>, which used models to generate text matching the style and information of existing data. With a title like that, we were hoping to reproduce and expand on the quality of the results that he had proclaimed.
                            </p>
                            <h2>Building from Scratch</h2>
                            <p class="lead">Having collected various text datasets that were roughly the same size as the ones that Kaparthy utilized (~10 mb), we set out to train our own models.</p>
                            <p class="lead">For training our models, we <a href="https://colab.research.google.com/drive/1WI-wbSyJ-d6I5YAJJCu2BiFqIxqFLN_l?usp=sharing", target="_blank">created a pipeline</a> that would allow us to easily swap in different model architectures, different datasets, and save both results and checkpoints for training. We also decided to focus on utilizing smaller levels of data input (characters) since we had previously tried to utilize a word-level approach in a previous assignment and found that it did not perform very well. We hypothesize that this is primarily due to working with a smaller dataset. Since we expected to be similarly constrained for this project as we were gathering fairly niche domains of input data, we chose to explore other possibilities of improvement instead of character input. Overall, here are some of the better performing models that we chose to utilize and are also hooked up into the backend here.</p>
                            <pre><code>
ScriptGenModelNLayer(
  (encoder): Embedding(111, 512)
  (lstm): LSTM(512, 512, num_layers=2, batch_first=True)
  (decoder): Linear(in_features=512, out_features=111, bias=True)
)
seq length: 100, batch_size: 64, feat_size: 512, test_batch_size: 256, lr: 0.002, decay: 0.0005
office_transcript_end_scene_linebreak
                            </code></pre>
                            <pre><code>
ScriptGenModelNLayerDropout(
  (encoder): Embedding(102, 512)
  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)
  (decoder): Linear(in_features=512, out_features=102, bias=True)
)
seq length: 100, batch_size: 64, feat_size: 512, test_batch_size: 256, lr: 0.0025, decay: 0.0005
nova_nature_transcripts
                            </code></pre>
                            <p class="lead">
                                We found that these models would pick up easily on the simplest levels of semantic structure of the input data such as words, the spacing, and the line breaks which is quite impressive considering that the model is generating and learning these sentences at the character level.
                            </p>
                            <pre><code>
Epoch 1: In the wild...
e the the sot forit itis ict ats of the tope can and the the byou thens te the to allike ar ther and bout the weles got mandist and it it yo uce cor and the melcew the the theen it tit's takit that th
                            </code></pre>
                            <pre><code>
Epoch 14: In the wild...
be
the like the link he are the with room mark what
it was the supress they hurded on the distance what means to the dolage all start people we can at the next were all the process and
then you have
                            </code></pre>
                            <p class="lead">
                                We see that in just 14 epochs, the model has learned valid words that it can create with characters.
                            </p>
                            <p class="lead">
                                Some things we tried to improve this was tinkering with the learning rate, changing the sequence length, adding more layers, and also introducing dropout. However, the models still had difficulty picking up more nuanced semantic aspects of the language. For the models that we had trained, we found that the testing accuracy was often stuck at the 60% range. Since accuracy measurement of text generation is still a very imprecise task, we would also print out periodic samples to see if the results improved by our human standards of language.
                            </p>
                            <p class="lead">
                                We saved the plots from each of our models, and most of them would taper around this 60% accuracy. Here are the test perplexity and training loss plots for the model we trained on The Office.
                            </p>
                            <img src="static/graphs/test_perplexity.png" alt="Test Perplexity for Office 3.1 Model" style="width:100%">
                            <img src="static/graphs/train_loss.png" alt="Train Loss for Office 3.1 Model" style="width:100%">
                            <p class="lead">
                                Something else that we got to investigate was the digging inside the model to look at the weight activations. While the majority of the weights were pretty randomly scattered, we did find certain weights that we could attach some sort of meaning to. For instance, cell 625 would be active for the dialogue part of the generated output. 
                            </p>
                            <img src="static/graphs/office3_1_activations.png" alt="Train Loss for Office 3.1 Model" style="width:100%">
                            <br>
                            <p class="lead">
                                Another avenue we were curious about was using GAN’s to allow the model to generate text that could be passed off as original data. We did not get to try this for ourselves, and looking into the literature such as <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2737434.pdf", target="_blank">this paper</a>, this direction did not seem as auspicious and is quite finicky to pull off. (GAN’s alone are already very difficult to reproduce).
                            </p>
                            <br>
                            <h2>The Alternative: <a href="https://towardsdatascience.com/fine-tuning-gpt2-on-colab-gpu-for-free-340468c92ed" target="_blank">Fine-tuning GPT2</a></h2>
                            </p>
                            <p class="lead">
                                The current trend in text-generation such as seen in GPT3 and Turing NLG is to simply drastically increase the number of parameters and input data. The usage of attention and transformers is also a central element of these more powerful models, and we obviously could not emulate the work here from scratch. In fact, even the transformer approach instead of an LSTM is hard to justify since this technique shines best on larger datasets. So instead, we fine tuned an available GPT2 model on our small datasets. We saved some of the models that we had tuned <a href="https://drive.google.com/drive/folders/1NKta8Fop8oTwiK8lJHVVefj5GpNfRT7U?usp=sharing" target="_blank">here</a>, but due to the large size and computation nature, you will probably have to run it yourself on collab or on a GPU.
                            </p>
                            <p class="lead">
                                Here are some samples of GPT2 trained fine-tuned on different datasets.
                            </p>
                            <pre style="max-height: 30vh">Trained on the PBS documentaries. <a href="https://drive.google.com/file/d/1BanqHVe42rwemqCMl5EXi1mILKGduMPi/view?usp=sharing" target="_blank">(nova_ALL_transcripts.txt)</a> <code>
Because the 1972 Chevy Avalanche was one of the most expensive cars in the world, it was the perfect time to get into auto collecting, because the Avalanche was already on its way to becoming the most expensive car in the world, a testament to the power and luxury that went with a car that. Chevrolet was in a tough position. After all, the Avalanche was a commercial success, a big hit with the public and a huge risk. But it was the Avalanche that kept them coming back.
ALEX'S MOM: I don't want her alive. I don't want her alive.
__: When they let her go, Alexander got a whole lot of shit done. Completely destroyed. Everything. Everything. It was like, they never thought of a way that they could get out of this.
__: I don't want her alive. I don't want her alive. I think she was taken advantage of. You know, she was a victim.
__: She had all the hallmarks of a victim. She had—That's the thing, we're going to show you in a second.
__: The hallmarks of a victim.
__: Lift, sink, free, dead center.
__: Lift, sink, free, dead center.
__: Lift, sink, free, dead center.
__: Lift, sink, free, dead center.
__: Lift, sink, free, dead center.
__: Lift, sink, free, dead center.
__: All right. We're going to take a left, and we're going to free, dead center.
__: We only know we've got one chance here.
__: All right. We'll take a left, and we're going to free, dead center.
__: We've got one chance here.
__: All right. We've got one chance here.
__: All right, now we have to get into the throttle.
__: All right. We've got to free, dead center.
__: It's a hell of a turn. We pull out, and we've got two more laps to go.
__: Four laps, two laps, dead center.
__: That was a snap. We picked up a little corner here and there. It was just a matter of, we had enough here to keep the little guy from getting into it, and it worked out fine.
__: Let's see what we can do.
__: Four laps, two laps, dead center.
__: That was a snap. We picked up the little corner here and there. It was just a matter of, we had enough here to keep the little guy from getting into it, and it worked out fine.
__: Four laps, two laps, dead center.
__: That was a snap. We pulled out, we've got two laps here and there to go.
__: Look at that.
__: That was a snap. We pulled out. It worked out great.
__: Let's see if we can corner and get the car into the corner.
__: Look at that.
__: That was perfect.
__: Let's make it.
__: That was perfect.
__: Let's make it.
__: Yay! (laughter)
__: We've got the throttle up and we've got the car in the corner.
__: All right. Let's get out there, and if we get by at all costs, then we should be able to—We've got to make it two cornering corners now, otherwise it's going to be a long wait. (laughter)
__: Wait a minute.
__: Wait a minute.
__: We could corner, we could—
__: We could corner.
__: We could corner.
__: We could corner.
__: Let's make it two cornering corners.
__: Yay! (laughter)
__: We're going to make it two cornering corners.
__: That was perfect.
__: That was perfect.
__: Let's make it.
__: That's the power.
__: That was perfect. That was the confidence. That was the confidence of, you know, 'cause I know that no other car driver has won that kind of margin before. No other driver has won that kind of a margin in any of the series where we have to go underneath it to make a corner.
__: Oh-ah! (laughter)
__: That was absolutely perfect.
__: Oh-ah! (laughter)
__: That was perfect.
__: That was perfect.
__: Let's make it.
__: That's all we have left.
__: Three laps.
__: That was perfect.
NARRATOR: The next race
                            </code></pre>
                            <pre style="max-height: 30vh">Trained on Mark Rober<a href="https://drive.google.com/file/d/1qTCOlxIG9tgYaP8g92O7i9CGph2qIQf4/view?usp=sharing" target="_blank">(cleaned_mark_rober_linebreak.txt)</a>
poverty and we've been using this to our advantage because it was a perfect fit, it's sort of the
Lightweight Standard from Last mile, and it weighs in at just under six ounces and it has a capacity of
260 grams. That's less than the two or three I used, but it does help us with some important panel
ingrations since we utilize them as standard
So with last mile's super smart striker system, we don't really need to ask the battery
person to fill up a cloud and the battery automatically goes away after a couple minutes
But if we need to charge something else, that charge is completely free and it simply needs to be on like 40% charge
So that's basically what I'm showing here, if I can fill up 40% charge with a little Rain de Whip and then there's no need to go to the store
We can simply rent a full charge port from Amazon for just over ten bucks and fill it with unlimited Rain de Wines Rain Guard
And then when we're ready to charge, we can simply go to setup and then use the magic of the Internet
to do exactly that, all right, so we've established Rain de Wines Rain Guard has 24 hours pre-filled order
I'm going to show you what it's like to actually get some
food for thought. First I'm going to demonstrate how to create a single serving
per week in moderation with no meat attached. And that provides us with some time to think
Before we get into this sciencey kind of
Scientifically we need to establish a number of principles so first of all
that's not proper diet is a fake scientific method
There's food in there that's artificial in taste or smell or taste
There's test tubes or something similar you name it
As long as you take these things as scientists
You're just trying to do science
like all the other people's babies these days
It's convenient easy to stick in the car
You have no idea when your food will actually be in the tube
So what the heck does this all mean? Basically what does this all mean if you don't take these things as scientists
Take-Home Lane Bea is wrong. The science is still pretty solid
Not only is there food in this tube, but we have artificial sweetener
Which is basically what is in this packet
And this leaves us no time to do our scientific method and to figure out what
All of these guys are eating is a waste of food and I know you're like
I totally get the point of the grocery store
But it's a waste of money they're putting food on this shelf for
Free which in hindsight probably would have been better spent adding
Sound effects perhaps? Maybe in hindsight probably would have been better spent
Removing parasites from fresh food that haven't evolved much in 5 million years because
They're dead so they can't pass on to your gut the nutrients it took to make your
Hungry PA is a gross overreaction but parasites aren't that hard to catch and it's easy to eat
So while I wasn't in this for the science
I was in this because it looked as if someone was intentionally eating a piece of food that wasn't their own food
And I wanted to test if this was a waste of money
The answer's a clear sell and I walk out the store
To get a refund if you just want to skip the first three checkout line by a penny
And then there's like two minutes of checkout in the middle
I still think this is a very good buy I
It's really simple
1.5 stars out of 5 stars out of 5
I haven't tried this yet but maybe in hindsight
You know I didn't know this would happen this fast this could be a problem because some restaurants have
Tiny plates on each side that are covered in food coloring to help keep the front
Car from reflecting the Sun the challenge here is as simple as adding extra calories or salt or
Phosphorus to the bottom of the plate
So let's do a little experiment to see how much higher the
calories or calories that food actually contains so I've got three samples
I'm going to take right now to my neighbor's party and just to prove how low you can go
I'm going to add two cups of red wine per sample
to the bottom of the six-ounce bottle and then I'm going to assume
that there's a 50/50 chance that the sample will be too hot
I'm just gonna add a little extra liquid now that the samples have been absorbed
it's time to see what happens
The first thing I'm gonna do is I'm gonna pour this out in two drops
the bottle should still be sparkling but I'm gonna set a timer to
Measure 12 o'clock but now I'm gonna add two cups of red
Reservoir fluid per sample
So that should take about 15 minutes so it's really
                            </code></pre>
                            <p class="lead">
                                It was interesting to contrast the results of GPT2 with those of our own models. Very clearly, the GPT2 model had a better grasp of the english language, but we noted that training too long on the dataset could cause the model to simply memorize entire lines of text. Also, if the seed words were very outside the domain of our dataset, GPT2 would sometimes generate text using its previous knowledge instead of matching the data that we had given it.
                            </p>
                            <h2>Some remarks</h2>
                            <p class="lead">
                                In summary, we definitely won't be challenging the dominance of giant attention based transformer networks anytime soon. Just curating and training on what seemed like would be a large dataset (ex: documentary transcripts) was a very tedious and time-consuming task. Instead, utilizing these existing models and fine-tuning produced the best results. Nonetheless, for this simplistic model, it is quite impressive how quickly it can pick up on these small cues. Additionally, there are advantages of having a smaller model when used in a practical deployment situation.
                            </p>
                            <br>
                        </div>
                    </div>

                    <div class="tab-pane fade" id="v-pills-speech" role="tabpanel" aria-labelledby="v-pills-speech-tab">
                        <div>
                            <h1>Text-to-Speech Synthesis</h1>
                            <p class="lead">Text-to-Speech synthesis was a completely new area for us! We decided to tackle this challenge in hopes of producing synthesized speech from our generated transcripts in the style of our targeted speakers. So, we researched and tested several different text-to-speech models.</p>
                            <p class="lead">Overall, we learned that text-to-speech synthesis generally follows a three part model. First, the text needs to be embedded using an encoder. Then, these embeddings are passed to a synthesizer that infers mel spectrograms from the word embeddings. Since we didn’t have much experience with sound, <a href="https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53" target="_blank">wrapping our heads around the idea of a mel spectrogram was pretty difficult</a>, but in essence, a mel spectrogram encodes signal frequencies from audio over time using the mel scale. Finally, the mel spectrograms are passed through a vocoder, which generates waveforms from the spectrograms, producing audio!</p>
                            <p class="lead">Below are the results of our journey!</p>
                
                            <br>
                            <h2>Experiment 1: <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" target="_blank">Real Time Voice Cloning</a></h2>
                            <p class="lead">We started by working with CorentinJ’s very popular Real-Time-Voice-Cloning repo. This repository was an implementation of transfer learning for a text-to-speech model. We iterated on the Jupyter notebook in this repository and tried training on clips of our speakers for voice cloning.</p>
                            <p class="lead">Initially, we hypothesized that attempting to voice clone using more data would lead to better results. So, we scraped audio from Stuart Reges’s CSE 143 lectures in Winter 2020. Not only was working with these massive .wav files exceptionally slow and memory-consuming (we had 30+ hours of data), but we discovered that this model (and most other models) were optimized to train on short ~10 second clips rather than long chunks of voice.</p>
                            <p class="lead">After downsampling a 10 second clip of Stuart’s voice to 48 kHz (to match the pretrained model), we attempted to generate his iconic lecture intro. While the generated voice sounded vaguely like Stuart, the output often had odd silences and white noise interruptions that were distracting. This was a great start, but the speech didn’t sound quite realistic for our liking. Since we had so much more data, we decided to look for ways that we could train a model using more than a 10 second clip to see if we could get improved results.</p>
                            <p class="lead">
                                Generated speech from 10 second Stuart clip, downsampled to 48kHz:
                                <br>
                                "Ok, let's go ahead and get started"
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/cloning-stuart-get-started-48000.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Check out our <a href="https://drive.google.com/file/d/1X5Xf-liLXdl83mb7RE54Wb_6RgmfO1ys/view?usp=sharing" target="_blank">voice cloning notebook</a> to try it out for yourself!</h2></p>
                                
                            <br>
                            <h2>Experiment 2: <a href="https://github.com/r9y9/wavenet_vocoder" target="_blank">WaveNet Vocoder</a></h2>
                            <p class="lead">Next, we looked for ways to train our own vocoder. We theorized that if we could get a vocoder fine-tuned on our speaker, we could pass mel spectrograms generated from preexisting models like Tacotron2 and output speech in the style of our speaker.</p>
                            <p class="lead">This repository also looked like a good place to start since it was structured to be able to train from a folder containing all .wav files from the targeted speaker. Learning from last time, we split up Stuart’s voice into 10 second chunks.</p>
                            <p class="lead">Unfortunately, we never got training working here. We ran into issues with incompatibility with training locally on Windows and Colab due to process spawning. We additionally tried running on attu since this was the only Linux system we had, but encountered issues due to memory limits..</p>
                            <p class="lead">
                                View our <a href="https://colab.research.google.com/drive/1ui3X-V-tSxnRG3wnr2VDZ4D_iIVXAJLV?usp=sharing" target="_blank">Wavenet notebook</a> to see our struggles.</h2>
                                <p class="lead">
                            </p>
                
                            <br>
                            <h2>Experiment 3: <a href="https://github.com/NVIDIA/waveglow" target="_blank">WaveGlow</a></h2>
                            <p class="lead">We found WaveGlow, which similarly is a vocoder that can synthesize speech from mel spectrograms. We tried running the pretrained models in this repository to synthesize speech from mel spectrograms of Stuart. The model was trained on a different sampling rate, so our output sounded very slow.</p>
                            <p class="lead">However, when we attempted to train our own model, we ran into more incompatibility issues with Colab and libraries that the training relies on. Setup is rough :((</p>
                            <p class="lead">
                                Here's our <a href="https://colab.research.google.com/drive/1HC6XugC0n61nnzlfRp4VxETf6NRi7pp6?usp=sharing" target="_blank">WaveGlow notebook</a> for more setup fun.</h2>
                            </p>
                
                            <br>
                            <h2>Experiment 4: <a href="https://github.com/NVIDIA/flowtron" target="_blank">Flowtron</a></h2>
                            <p class="lead">Finally, we found Flowtron through <a href="https://developer.nvidia.com/blog/training-your-own-voice-font-using-flowtron/" target="_blank">this blog post</a></h2> and were excited about the discussion of style transfer! Flowtron takes input text, encodes embeddings of these words, and decodes these embeddings to produce mel spectrograms. Then, the mel spectrograms can be decoded into waveforms using vocoders like Waveglow. Since we ran into so many training issues when trying to train a vocoder, we focused on training Flowtron instead and used a pretrained Waveglow model.</p>
                
                            <p class="lead">Flowtron network structure (excluding include text encoder and gate layer, source <a href="https://developer.nvidia.com/blog/training-your-own-voice-font-using-flowtron/" target="_blank">here</a>)<img src="static/graphs/flowtron-diagram.png" alt="Flowtron Diagram" style="width:100%"></p>
                
                            <p class="lead">We realized that in order to train this speech synthesis model end-to-end, we needed audio files along with transcribed text of what the speaker was saying. Initially, we were worried that this was a hurdle we wouldn’t be able to overcome, since we weren’t sure how we could find such extensive data sets on the speakers we were interested in. However, we realized that YouTubers that include human transcribed captions in their videos could be a source of this data! .vtt files include timestamps for each caption text, so we were able to parse the caption files and create a dataset of short audio clips from Kurzgesagt along with the corresponding text.</p>
                
                            <p class="lead">We finally were able to get a model training, which was super exciting!! We had about 3 hours of Kurzgesagt data, so based on the recommendations in the blog, we decided to work from a pretrained model and try both fine-tuning the last layer and training the last layer from scratch. We started transfer learning from a model trained on the <a href="https://keithito.com/LJ-Speech-Dataset/" target="_blank">LJS dataset</a>.</p>
                
                            <br>
                            <h3>Experiment 4.1: Fine-tuning pretrained Flowtron</h3>
                            <p class="lead">When fine-tuning the model, we saw the validation loss decrease from ~0.9 to 0.32, which seemed initially promising. However, when we ran text-to-speech using our model, we found that the resulting audio was not very impressive. The speech sounded garbled, and more like someone practicing their vowels for the first time than actual words. The speaking voice sounded feminine (since the pretrained model was female voice) and not at all like Kurzgesagt. Upon further investigation, we realized that the loss function utilized negative log likelihood, so in fact these loss values didn’t seem that promising!</p>
                            <p class="lead">
                                Generated speech from our finetuned model after 22.75k iterations:
                                <br>
                                “Oh man, finals week is a little rough.”
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/flowtron-kurzgesagt-finals-finetune3-model22750.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Finetuned training loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-finetune-training_loss.svg" alt="Flowtron Finetuned Training Loss">
                            <br>
                            <br>
                            <p class="lead">Finetuned validation loss (negative log likelihood)  vs. Iterations</p>
                            <img src="static/graphs/flowtron-finetune-validation_loss.svg" alt="Flowtron Finetuned Validation Loss">
                
                            <br>
                            <br>
                            <h3>Experiment 4.2: Training the last layer from scratch</h3>
                            <p class="lead">We next tried training the last layer from scratch. We saw our validation loss initially decrease to -0.81 after ~3.5k iterations which seemed a lot more reasonable! However, again we noticed that the attention visualizations did not look crisp and clear. When we tried synthesizing audio, the speech was much noisier than before and sounded much more raw and unrefined. However, we did notice that the speaker’s voice sounded more masculine, and possibly closer to Kurzgesagt. Additionally, we were able to hear a snippet of coherent words coming through (listen for "finals week" in the audio clip below), unlike the babbling vowels from our previous model.</p>
                            <p class="lead">As we continued training, we saw the validation loss rapidly increase as the model began to overfit. According to Flowtron’s documentation, we should have had enough data to train the model, so we were likely overfitting to noise in our audio.</p>
                            <p class="lead">
                                Generated speech after training the last layer from scratch at 3.25k iterations:
                                <br>
                                “Oh man, finals week is a little rough.”
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/flowtron-kurzgesagt-finals-ignore1-model3250.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Last layer from scratch training loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-ignore-training_loss.svg" alt="Layer From Scratch Training Loss">
                            <br>
                            <br>
                            <p class="lead">Last layer from scratch validation loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-ignore-validation_loss.svg" alt="Last Layer From Scratch Validation Loss">
                
                            <br>
                            <br>
                            <h3>Flowtron Conclusions</h3>
                            <p class="lead">Ultimately, both of our transfer learning models were pretty unusable which was a little disappointing (although the synthesized “speech” is pretty entertaining). We theorize that our dataset wasn’t good enough. Although we were able to get correct transcriptions for each audio clip using the captions we scraped, the audio clips likely had too much background noise due to music and sound effects in the videos. We were unable to find a good way to clean this data (and trying to do so would probably involve another deep learning task anyway, which leads to even more room for error). We learned how important it is to have good, clean data, and how hard it is to find the data you want! It makes sense that we only saw a couple popular datasets being used like LJS and <a href="https://research.google/tools/datasets/libri-tts/" target="_blank">LibriTTS</a>, since it takes a lot of work to create quality data sets like these and they are pretty rare!</p>
                
                            <p class="lead">View our Flowtron Juptyer notebook <a href="https://colab.research.google.com/drive/1_2h6Y7aUXk2b1Vw3100r6T4ZjbW8m9QC?usp=sharing " target="_blank">here!</a></p>
                        </div>
                    </div>
                    </div>

               </div>
            </div>
        </div>
    </div>

    <footer class="page-footer"></footer>

</main>

<script>
    window.addEventListener('scroll',function() {
        //When scroll change, you save it on localStorage.
        localStorage.setItem('scrollPosition',window.scrollY);
    },false);

    window.addEventListener('load',function() {
        if(localStorage.getItem('scrollPosition') !== null)
           window.scrollTo(0, localStorage.getItem('scrollPosition'));
    },false);
</script>

<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>
{% endblock %}
