{% extends "base.html" %}

{% block head %}
    <!-- Required meta tags -->
    
    <link rel="stylesheet" href="{{ url_for('static', filename='css/index.css') }}">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

{% endblock %}

{% block content %}
<main role="main">

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <section class="jumbotron text-center">
      <div class="container">
        <h1 class="display-3">Speech-to-Speech:</h1>
        <h3 class="display-4">Realistic Audio Reading of <br>Generated Transcripts</h2>
        <br>
        <p>Arthur Liu, Taylor Ka, Daniel Zhu</p>
      </div>
    </section>

    <div class="container">
        <div class="row">
            <div class="col-3">
                <div class="nav flex-column nav-pills" id="v-pills-tab" role="tablist" aria-orientation="vertical">
                    <a class="nav-link active" id="v-pills-demo-tab" data-toggle="pill" href="#v-pills-demo" role="tab" aria-controls="v-pills-demo" aria-selected="true">1 Demo</a>
                    <a class="nav-link" id="v-pills-overview-tab" data-toggle="pill" href="#v-pills-overview" role="tab" aria-controls="v-pills-overview" aria-selected="false">2 Overview</a>
                    <a class="nav-link" id="v-pills-dataset-tab" data-toggle="pill" href="#v-pills-dataset" role="tab" aria-controls="v-pills-dataset" aria-selected="false">3 Dataset Creation</a>
                    <a class="nav-link" id="v-pills-language-tab" data-toggle="pill" href="#v-pills-language" role="tab" aria-controls="v-pills-language" aria-selected="false">4 Language Models</a>
                    <a class="nav-link" id="v-pills-speech-tab" data-toggle="pill" href="#v-pills-speech" role="tab" aria-controls="v-pills-speech" aria-selected="false">5 Speech Synthesis </a>
                </div>
            </div>
    
            <div class="col-9">
                <div class="tab-content" id="v-pills-tabContent">
                    <div class="tab-pane fade show active" id="v-pills-demo" role="tabpanel" aria-labelledby="v-pills-demo-tab">
                        <div>
                            <p class="lead">
                                Now that the quarter is almost over... aren't you going to miss listening to all your professors lecture? No? Maybe you've just watched all the Youtube videos
                                from your favorite vlogger? Are you sitting there pulling down to refresh waiting for new content? <b>Well regardless... you don't have to miss a beat!</b>  
                            </p>
                            <p class="lead">
                                Speech-to-Speech generates new video scripts and uses the original creator's voice to produce a realistic audio reading. 
                            </p>
                            <h3>Try it yourself!</h3>
                            <br>
                            <form>
                                <div class="form-group lead">
                                    <label for="seedWords">Seed Words</label>
                                    <input type="text" class="form-control" name="seedWords" id="seedWords" placeholder="Enter seed words">
                                </div>
                                <div class="form-row">
                                    <div class="col">
                                        <p class="lead">Text Generator</p>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="markRober" value="mark">
                                            <label class="form-check-label" for="markRober">
                                              Mark Rober
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="caseyNeistat" value="casey">
                                            <label class="form-check-label" for="caseyNeistat">
                                              Casey Neistat Vlogs
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="office" value="office">
                                            <label class="form-check-label" for="office">
                                              The Office
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="documentary" value="documentary">
                                            <label class="form-check-label" for="documentary">
                                              PBS Documentaries
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="languageModel" id="nature" value="nature">
                                            <label class="form-check-label" for="nature">
                                              Nature Documentary
                                            </label>
                                        </div>
                                    </div>
                                    <div class="col">
                                        <p class="lead">Audio Reader</p>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="audioReader" id="stuartRegesVoice" value="stuartRegesVoice">
                                            <label class="form-check-label" for="stuartRegesVoice">
                                              Stuart Reges
                                            </label>
                                        </div>
                                        <div class="form-check">
                                            <input class="form-check-input" type="radio" name="audioReader" id="caseyNeistatVoice" value="caseyNeistatVoice">
                                            <label class="form-check-label" for="caseyNeistatVoice">
                                              Casey Neistat
                                            </label>
                                        </div>
                                    </div>
                                </div>
                                <br>
                                <button type="submit" class="btn btn-primary">Generate</button>
                            </form>
            
                            <br>
                            {% if text is not none %}
                                <h3>Generated Text</h3>
                                {% for p in text %}
                                    <p class="lead">{{ p }}</p>
                                {% endfor %}
                            {% endif %}
                    
                            {% if audio is not none %}
                                <h3>Audio Reading</h3>
                                <p class="lead">
                                    <audio controls>
                                        <source src="{{ audio }}" type="audio/wav">
                                    </audio>
                                </p>
                            {% endif %}
                    
                            {% if visualizations is not none %}
                                <h3>
                                    Visualizatizing the Neurons
                                </h3>
                                <p class="lead">
                                    How individual cells contribute to the predictions - Red = Higher Activity, Blue = Mostly Inactive
                                </p>
                                {% for visualization in visualizations %}
                                <p class="lead">
                                    Cell #{{ visualization[0] }}
                                </p>
                                <p class="lead">
                                    {{ visualization[1]|safe}}
                                </p>
                                {% endfor %}
                            {% endif %}
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-overview" role="tabpanel" aria-labelledby="v-pills-overview-tab">
                        <div>
                            <p class="lead">This is how we built the project: </p>
                            <!-- Created and copied from draw io -->
                            <div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile modified=\&quot;2020-12-13T13:19:38.832Z\&quot; host=\&quot;app.diagrams.net\&quot; agent=\&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\&quot; etag=\&quot;YQCihewNGoS7LejKZ34G\&quot; version=\&quot;14.0.1\&quot; type=\&quot;google\&quot;&gt;&lt;diagram id=\&quot;Ht1M8jgEwFfnCIfOTk4-\&quot; name=\&quot;Page-1\&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
                            <p class="lead"> In the following sections, we'll go into more detail about each of the 3 sections: dataset creation, text generation, and speech synthesis. Finally, we will discuss our results 
                                and compare the different models we experimented with.
                            </p>
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-dataset" role="tabpanel" aria-labelledby="v-pills-dataset-tab">
                        <div>
                            <h3>Dataset Creation</h3>
                            <p class="lead">The fun of this project comes from being able to style both the text generation and speech synthesis to people that we were familiar with. So, we collected this data online
                                from documentaries, Youtube, and online courseware. Sometimes, it was difficult to ensure that the transcripts of the videos we found were high quality, this was especially true
                                when we used auto-generated English captions from Youtube videos. All in all, we created a few datasets listed below:
                            <div class="col-8 mx-auto">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th scope="col">Name</th>
                                            <th scope="col">Source</th>
                                            <th scope="col">Size</th>
                                        </tr>
                                        <tbody>
                                            <tr>
                                                <td>Stuart Reges</td>
                                                <td>Idk how we got his past lectures</td>
                                                <td>Not sure</td>
                                            </tr>
                                            <tr>
                                                <td>Casey Neistat</td>
                                                <td>Youtube</td>
                                                <td>how big</td>
                                            </tr>
                                            <tr>
                                                <td>Planet Earth</td>
                                                <td>Subslikescript</td>
                                                <td>these were</td>
                                            </tr>
                                        </tbody>
                                    </thead>
                                </table>
                            </div>    
                            </p>
                        </div>
                    </div>
                    <div class="tab-pane fade" id="v-pills-language" role="tabpanel" aria-labelledby="v-pills-language-tab"></div>

                    <div class="tab-pane fade" id="v-pills-speech" role="tabpanel" aria-labelledby="v-pills-speech-tab">
                        <div>
                            <h1>Text-to-Speech Synthesis</h1>
                            <p class="lead">Text-to-Speech synthesis was a completely new area for us! We decided to tackle this challenge in hopes of producing synthesized speech from our generated transcripts in the style of our targeted speakers. So, we researched and tested several different text-to-speech models.</p>
                            <p class="lead">Overall, we learned that text-to-speech synthesis generally follows a three part model. First, the text needs to be embedded using an encoder. Then, these embeddings are passed to a synthesizer that infers mel spectrograms from the word embeddings. Since we didn’t have much experience with sound, <a href="https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53" target="_blank">wrapping our heads around the idea of a mel spectrogram was pretty difficult</a>, but in essence, a mel spectrogram encodes signal frequencies from audio over time using the mel scale. Finally, the mel spectrograms are passed through a vocoder, which generates waveforms from the spectrograms, producing audio!</p>
                            <p class="lead">Below are the results of our journey!</p>
                
                            <br>
                            <h2>Experiment 1: <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" target="_blank">Real Time Voice Cloning</a></h2>
                            <p class="lead">We started by working with CorentinJ’s very popular Real-Time-Voice-Cloning repo. This repository was an implementation of transfer learning for a text-to-speech model. We iterated on the Jupyter notebook in this repository and tried training on clips of our speakers for voice cloning.</p>
                            <p class="lead">Initially, we hypothesized that attempting to voice clone using more data would lead to better results. So, we scraped audio from Stuart Reges’s CSE 143 lectures in Winter 2020. Not only was working with these massive .wav files exceptionally slow and memory-consuming (we had 30+ hours of data), but we discovered that this model (and most other models) were optimized to train on short ~10 second clips rather than long chunks of voice.</p>
                            <p class="lead">After downsampling a 10 second clip of Stuart’s voice to 48 kHz (to match the pretrained model), we attempted to generate his iconic lecture intro. While the generated voice sounded vaguely like Stuart, the output often had odd silences and white noise interruptions that were distracting. This was a great start, but the speech didn’t sound quite realistic for our liking. Since we had so much more data, we decided to look for ways that we could train a model using more than a 10 second clip to see if we could get improved results.</p>
                            <p class="lead">
                                Generated speech from 10 second Stuart clip, downsampled to 48kHz:
                                <br>
                                "Ok, let's go ahead and get started"
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/cloning-stuart-get-started-48000.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Check out our <a href="https://colab.research.google.com/drive/1X5Xf-liLXdl83mb7RE54Wb_6RgmfO1ys?authuser=3#scrollTo=PHenc_Bdh9yh" target="_blank">voice cloning notebook</a> to try it out for yourself!</h2></p>
                                
                            <br>
                            <h2>Experiment 2: <a href="https://github.com/r9y9/wavenet_vocoder" target="_blank">WaveNet Vocoder</a></h2>
                            <p class="lead">Next, we looked for ways to train our own vocoder. We theorized that if we could get a vocoder fine-tuned on our speaker, we could pass mel spectrograms generated from preexisting models like Tacotron2 and output speech in the style of our speaker.</p>
                            <p class="lead">This repository also looked like a good place to start since it was structured to be able to train from a folder containing all .wav files from the targeted speaker. Learning from last time, we split up Stuart’s voice into 10 second chunks.</p>
                            <p class="lead">Unfortunately, we never got training working here. We ran into issues with incompatibility with training locally on Windows and Colab due to process spawning. We additionally tried running on attu since this was the only Linux system we had, but encountered issues due to memory limits..</p>
                            <p class="lead">
                                View our <a href="https://colab.research.google.com/drive/1ui3X-V-tSxnRG3wnr2VDZ4D_iIVXAJLV?usp=sharing" target="_blank">Wavenet notebook</a> to see our struggles.</h2>
                                <p class="lead">
                            </p>
                
                            <br>
                            <h2>Experiment 3: <a href="https://github.com/NVIDIA/waveglow" target="_blank">WaveGlow</a></h2>
                            <p class="lead">We found WaveGlow, which similarly is a vocoder that can synthesize speech from mel spectrograms. We tried running the pretrained models in this repository to synthesize speech from mel spectrograms of Stuart. The model was trained on a different sampling rate, so our output sounded very slow.</p>
                            <p class="lead">However, when we attempted to train our own model, we ran into more incompatibility issues with Colab and libraries that the training relies on. Setup is rough :((</p>
                            <p class="lead">
                                Here's our <a href="https://colab.research.google.com/drive/1HC6XugC0n61nnzlfRp4VxETf6NRi7pp6?usp=sharing" target="_blank">WaveGlow notebook</a> for more setup fun.</h2>
                            </p>
                
                            <br>
                            <h2>Experiment 4: <a href="https://github.com/NVIDIA/flowtron" target="_blank">Flowtron</a></h2>
                            <p class="lead">Finally, we found Flowtron through <a href="https://developer.nvidia.com/blog/training-your-own-voice-font-using-flowtron/" target="_blank">this blog post</a></h2> and were excited about the discussion of style transfer! Flowtron takes input text, encodes embeddings of these words, and decodes these embeddings to produce mel spectrograms. Then, the mel spectrograms can be decoded into waveforms using vocoders like Waveglow. Since we ran into so many training issues when trying to train a vocoder, we focused on training Flowtron instead and used a pretrained Waveglow model.</p>
                
                            <p class="lead">Flowtron network structure (excluding include text encoder and gate layer, source <a href="https://developer.nvidia.com/blog/training-your-own-voice-font-using-flowtron/" target="_blank">here</a>)<img src="static/graphs/flowtron-diagram.png" alt="Flowtron Diagram" style="width:100%"></p>
                
                            <p class="lead">We realized that in order to train this speech synthesis model end-to-end, we needed audio files along with transcribed text of what the speaker was saying. Initially, we were worried that this was a hurdle we wouldn’t be able to overcome, since we weren’t sure how we could find such extensive data sets on the speakers we were interested in. However, we realized that YouTubers that include human transcribed captions in their videos could be a source of this data! .vtt files include timestamps for each caption text, so we were able to parse the caption files and create a dataset of short audio clips from Kurzgesagt along with the corresponding text.</p>
                
                            <p class="lead">We finally were able to get a model training, which was super exciting!! We had about 3 hours of Kurzgesagt data, so based on the recommendations in the blog, we decided to work from a pretrained model and try both fine-tuning the last layer and training the last layer from scratch. We started transfer learning from a model trained on the <a href="https://keithito.com/LJ-Speech-Dataset/" target="_blank">LJS dataset</a>.</p>
                
                            <br>
                            <h3>Experiment 4.1: Fine-tuning pretrained Flowtron</h3>
                            <p class="lead">When fine-tuning the model, we saw the validation loss decrease from ~0.9 to 0.32, which seemed initially promising. However, when we ran text-to-speech using our model, we found that the resulting audio was not very impressive. The speech sounded garbled, and more like someone practicing their vowels for the first time than actual words. The speaking voice sounded feminine (since the pretrained model was female voice) and not at all like Kurzgesagt. Upon further investigation, we realized that the loss function utilized negative log likelihood, so in fact these loss values didn’t seem that promising!</p>
                            <p class="lead">
                                Generated speech from our finetuned model after 22.75k iterations:
                                <br>
                                “Oh man, finals week is a little rough.”
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/flowtron-kurzgesagt-finals-finetune3-model22750.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Finetuned training loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-finetune-training_loss.svg" alt="Flowtron Finetuned Training Loss">
                            <br>
                            <br>
                            <p class="lead">Finetuned validation loss (negative log likelihood)  vs. Iterations</p>
                            <img src="static/graphs/flowtron-finetune-validation_loss.svg" alt="Flowtron Finetuned Validation Loss">
                
                            <br>
                            <br>
                            <h3>Experiment 4.2: Training the last layer from scratch</h3>
                            <p class="lead">We next tried training the last layer from scratch. We saw our validation loss initially decrease to -0.81 after ~3.5k iterations which seemed a lot more reasonable! However, again we noticed that the attention visualizations did not look crisp and clear. When we tried synthesizing audio, the speech was much noisier than before and sounded much more raw and unrefined. However, we did notice that the speaker’s voice sounded more masculine, and possibly closer to Kurzgesagt. Additionally, we were able to hear a snippet of coherent words coming through (listen for "finals week" in the audio clip below), unlike the babbling vowels from our previous model.</p>
                            <p class="lead">As we continued training, we saw the validation loss rapidly increase as the model began to overfit. According to Flowtron’s documentation, we should have had enough data to train the model, so we were likely overfitting to noise in our audio.</p>
                            <p class="lead">
                                Generated speech after training the last layer from scratch at 3.25k iterations:
                                <br>
                                “Oh man, finals week is a little rough.”
                                <br>
                                <audio controls>
                                    <source src="static/audio-examples/flowtron-kurzgesagt-finals-ignore1-model3250.wav" type="audio/wav">
                                </audio>
                            </p> 
                            <p class="lead">Last layer from scratch training loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-ignore-training_loss.svg" alt="Layer From Scratch Training Loss">
                            <br>
                            <br>
                            <p class="lead">Last layer from scratch validation loss (negative log likelihood) vs. Iterations</p>
                            <img src="static/graphs/flowtron-ignore-validation_loss.svg" alt="Last Layer From Scratch Validation Loss">
                
                            <br>
                            <br>
                            <h3>Flowtron Conclusions</h3>
                            <p class="lead">Ultimately, both of our transfer learning models were pretty unusable which was a little disappointing (although the synthesized “speech” is pretty entertaining). We theorize that our dataset wasn’t good enough. Although we were able to get correct transcriptions for each audio clip using the captions we scraped, the audio clips likely had too much background noise due to music and sound effects in the videos. We were unable to find a good way to clean this data (and trying to do so would probably involve another deep learning task anyway, which leads to even more room for error). We learned how important it is to have good, clean data, and how hard it is to find the data you want! It makes sense that we only saw a couple popular datasets being used like LJS and <a href="https://research.google/tools/datasets/libri-tts/" target="_blank">LibriTTS</a>, since it takes a lot of work to create quality data sets like these and they are pretty rare!</p>
                
                            <p class="lead">View our Flowtron Juptyer notebook <a href="https://colab.research.google.com/drive/1_2h6Y7aUXk2b1Vw3100r6T4ZjbW8m9QC?usp=sharing " target="_blank">here!</a></p>
                        </div>
                    </div>
                    </div>

               </div>
            </div>
        </div>
    </div>

    <footer class="page-footer"></footer>

</main>

<script>
    window.addEventListener('scroll',function() {
        //When scroll change, you save it on localStorage.
        localStorage.setItem('scrollPosition',window.scrollY);
    },false);

    window.addEventListener('load',function() {
        if(localStorage.getItem('scrollPosition') !== null)
           window.scrollTo(0, localStorage.getItem('scrollPosition'));
    },false);
</script>

<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>
{% endblock %}
